# SnapAnalyst Environment Configuration (REQUIRED)
# =================================================
# This file is REQUIRED. The application will not start without it.
# Copy this file to .env and configure your LLM provider and API keys.
#
# Usage:
#   cp .env.example .env
#   # Edit .env with your LLM provider settings
#   docker-compose up -d

# =============================================================================
# Database Configuration
# =============================================================================
DATABASE_PASSWORD=your-secure-password-here
POSTGRES_PORT=5432

# =============================================================================
# Application Settings
# =============================================================================
ENVIRONMENT=production
LOG_LEVEL=INFO
DEBUG=false

# Security - CHANGE THESE IN PRODUCTION!
SECRET_KEY=change-this-to-a-secure-random-string
# CHAINLIT_AUTH_SECRET=change-this-to-a-secure-random-string

# =============================================================================
# Port Configuration
# =============================================================================
API_PORT=8000
CHAINLIT_PORT=8001

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose one: openai | anthropic | ollama
LLM_PROVIDER=azure_openai

# OpenAI (if using openai provider)
# OPENAI_API_KEY=


# Azure OpenAI Endpoint (from Azure Portal → Your Resource → Keys and Endpoint)
# AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/openai/v1/
# Azure OpenAI API Key (from Azure Portal → Your Resource → Keys and Endpoint)
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here

# Anthropic (if using anthropic provider)
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Ollama (if using ollama provider)
# For Docker on Mac/Windows, use host.docker.internal to access host machine
# For Docker on Linux, use the host's IP address or set up host networking
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# =============================================================================
# LLM Model Settings (Optional - uses provider defaults if not set)
# =============================================================================
# SQL Generation Model
LLM_SQL_MODEL=gpt-4.1                    # OpenAI
# LLM_SQL_MODEL=claude-sonnet-4-20250514 # Anthropic
# LLM_SQL_MODEL=llama3.1:8b              # Ollama
LLM_SQL_MAX_TOKENS=2000
LLM_SQL_TEMPERATURE=0.1

# Insights & Knowledge Base Model (used as default for both /? and //?? modes)
# Can use a smaller/faster model since these don't generate SQL
LLM_KB_MODEL=gpt-4.1-mini                # OpenAI
# LLM_KB_MODEL=claude-haiku-4-5-20251001 # Anthropic
# LLM_KB_MODEL=llama3.1:8b               # Ollama

# Max response tokens for insights (default: 1000, max: 8000)
# Higher = longer, more detailed insight answers
LLM_KB_MAX_TOKENS=1000

# Max chars for previous query data included in insights (default: 50000)
# With large context models (128k+), increase for richer thread analysis
LLM_KB_MAX_DATA_SIZE=50000

# Total max prompt size in chars (default: 100000)
# Should accommodate system prompt + KB docs + thread data
LLM_KB_MAX_PROMPT_SIZE=100000

LLM_KB_TEMPERATURE=0.1

# =============================================================================
# AI Summary for SQL Results (Optional)
# =============================================================================
# Enable AI-powered natural language summaries of query results (default: true)
# When enabled, the LLM analyzes results and provides insights
# When disabled, simple template-based summaries are used (e.g., "Found 42 results.")
LLM_SQL_SUMMARY_ENABLED=true

# Max result rows sent to the LLM for summary generation (default: 50)
# More rows = better summary but more tokens. Scales automatically with context window.
LLM_SQL_SUMMARY_MAX_ROWS=50

# =============================================================================
# Vanna RAG Retrieval (Optional)
# =============================================================================
# Number of SQL example pairs retrieved from ChromaDB per query (default: 10)
# More examples = better SQL generation for complex queries
VANNA_N_RESULTS_SQL=10

# Number of documentation chunks retrieved from ChromaDB per query (default: 10)
# More docs = better understanding of business context and schema
VANNA_N_RESULTS_DOCS=10

# SQL GENERATION AND TABLE OPTIONS
# MAX_RESULT_COLUMNS=10


# =============================================================================
# Performance Tuning (Optional)
# =============================================================================
# ONNX Runtime optimization for containerized environments
# IMPORTANT: These settings prevent CPU affinity errors in Docker/LXC containers
# See: https://github.com/chroma-core/chroma/issues/1420
#
# OMP_NUM_THREADS: Thread pool size (default: 4)
# ORT_DISABLE_CPU_EP_AFFINITY: Disable CPU execution provider affinity (default: 1)
# ORT_DISABLE_THREAD_AFFINITY: Disable thread affinity (default: 1)
# Recommended: Keep defaults unless experiencing threading issues
# OMP_NUM_THREADS=4
# ORT_DISABLE_CPU_EP_AFFINITY=1
# ORT_DISABLE_THREAD_AFFINITY=1

# =============================================================================
# Vanna Training Data (Optional)
# =============================================================================
# Path to folder containing SQL training data for Vanna AI.
# All .md/.txt files are loaded as documentation, all .json as question/SQL pairs.
# See README.md "Training Data Folder" section for details.
# SQL_TRAINING_DATA_PATH=./datasets/snap/training

# =============================================================================
# System Prompts (Optional)
# =============================================================================
# Path to folder containing LLM system prompts.
# Files: sql_system_prompt.txt (SQL generation), kb_system_prompt.txt (KB insights), summary_system_prompt.txt (AI result summaries)
# If missing, generic dataset-agnostic defaults are used.
# SYSTEM_PROMPTS_PATH=./datasets/snap/prompts
