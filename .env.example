# SnapAnalyst Docker Environment Configuration
# =============================================
# Copy this file to .env and update the values for your deployment.
#
# Usage:
#   cp .env.example .env
#   # Edit .env with your settings
#   docker-compose up -d

# =============================================================================
# Database Configuration
# =============================================================================
DATABASE_PASSWORD=your-secure-password-here
POSTGRES_PORT=5432

# =============================================================================
# Application Settings
# =============================================================================
ENVIRONMENT=production
LOG_LEVEL=INFO
DEBUG=false

# Security - CHANGE THIS IN PRODUCTION!
SECRET_KEY=change-this-to-a-secure-random-string

# =============================================================================
# Port Configuration
# =============================================================================
API_PORT=8000
CHAINLIT_PORT=8001

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose one: openai | anthropic | ollama
LLM_PROVIDER=openai

# OpenAI (if using openai provider)
# OPENAI_API_KEY=


# Azure OpenAI Endpoint (from Azure Portal → Your Resource → Keys and Endpoint)
# AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/openai/v1/
# Azure OpenAI API Key (from Azure Portal → Your Resource → Keys and Endpoint)
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here

# Anthropic (if using anthropic provider)
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Ollama (if using ollama provider)
# For Docker on Mac/Windows, use host.docker.internal to access host machine
# For Docker on Linux, use the host's IP address or set up host networking
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# =============================================================================
# LLM Model Settings (Optional - uses provider defaults if not set)
# =============================================================================
# SQL Generation Model
LLM_SQL_MODEL=gpt-4.1      # OpenAI
# LLM_SQL_MODEL=claude-3-5-sonnet-20241022 # Anthropic
# LLM_SQL_MODEL=llama3.1:8b               # Ollama
LLM_SQL_MAX_TOKENS=2000
LLM_SQL_TEMPERATURE=0.1

# Summary Generation Model (can use smaller/faster model)
LLM_KB_MODEL=gpt-3.5-turbo         # OpenAI

# Max response tokens for insights (default: 150)
LLM_KB_MAX_TOKENS=150

# Max chars for previous query data included in insights (default: 8000)
# Increase if you want more data context in insights
LLM_KB_MAX_DATA_SIZE=8000

# Total max prompt size in chars (default: 10000)
LLM_KB_MAX_PROMPT_SIZE=10000

LLM_KB_TEMPERATURE=0.1 

# SQL GENERATION AND TABLE OPTIONS
# MAX_RESULT_COLUMNS=10


# =============================================================================
# Performance Tuning (Optional)
# =============================================================================
# ONNX Runtime thread pool size (default: 4)
# IMPORTANT: Must be set to a non-zero value to prevent CPU affinity errors
# in LXC containers (Proxmox, etc.). When explicitly set, ONNX Runtime skips
# automatic CPU affinity which fails in containerized environments.
# Recommended: 4 for most systems, 8-16 for high-CPU systems
# See: https://github.com/chroma-core/chroma/issues/1420
# OMP_NUM_THREADS=4

# =============================================================================
# Vanna Configuration (Optional)
# =============================================================================
# Enable Vanna training on startup (can slow down initial startup)
VANNA_TRAINING_ENABLED=false
DATABASE_URL=postgresql://snapanalyst:your-secure-password-here@localhost:5432/snapanalyst_db
CHAINLIT_AUTH_SECRET="change-this-to-a-secure-random-string"
